For my personal projects, I often get inspired by research papers I read about human-computer interaction.

Lately, the one I've been looking into is called " [Ubicoustics - Plug-and-play acoustic activity recognition](http://www.gierad.com/assets/ubicoustics/ubicoustics.pdf)" by a team of researchers at the CMU (Carnegie Mellon University) in the US.

**Acoustic activity recognition is using the rich properties of sound to gain insights about an environment or activity.**

This can be used to enhance smart systems and build more personalised connected homes.

Researchers at the CMU used Python to prototype their experiments and made their [project open-source on Github](https://github.com/FIGLAB/ubicoustics) if you want to have a look but I wanted to see if I could reproduce something similar using web technologies, and it worked! 😃

The end result is a prototype of browser-based acoustic activity recognition system, classifying speaking, coughing, typing, brushing teeth and my phone ringing:

[![Acoustic activity recognition using the microphone](https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fres.cloudinary.com%2Fdevdevcharlie%2Fimage%2Fupload%2Fv1575157529%2Factivities2_wzux1a.gif)](https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fres.cloudinary.com%2Fdevdevcharlie%2Fimage%2Fupload%2Fv1575157529%2Factivities2_wzux1a.gif)

_This is still early on in my explorations but I thought I'd share what I've learnt so far._

## Why using sound?

A lot of devices around us have a built-in microphone; your laptop, phone, tablet, smart watch, home assistant, etc... however, they don't really leverage the rich properties of sound.

In general, applications listen for a certain word to trigger actions like "Ok, Google" or "Alexa", but words are not the only thing that produce distinguishable sounds; everything around us generates sounds.

If you take a second to think about it, you know what the sound of rain is, you know the difference between what a fridge sounds like when it's being opened versus a microwave, you recognise the sound of a doorbell even if it's not yours, etc...

I like to think that if your brain is capable of taking sound input and classifying it, then something similar should be possible using machine learning; so let's see how this would work.

## Tech stack

For this prototype, I used the **[Web Audio API](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API)** to use the microphone as input, **[Canvas](https://developer.mozilla.org/en-US/docs/Web/API/Canvas_API)** to build a spectrogram with the sound data and **[Tensorflow.js](https://www.tensorflow.org/js)** to train a model to recognise activities.

To make it easier, I used the **[Teachable machine](https://teachablemachine.withgoogle.com/)** experiment by Google to record my sound samples, train the machine learning model and export it.

Now, let's go through some of the steps I took to build this.

## Visualising sound data

When you inspect the data you get from the microphone input using the Web Audio API, you get something like this:

[![Sound data logged in the console as array of integers](https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fres.cloudinary.com%2Fdevdevcharlie%2Fimage%2Fupload%2Fv1576912095%2Funnamed_1_qostnv.gif)](https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fres.cloudinary.com%2Fdevdevcharlie%2Fimage%2Fupload%2Fv1576912095%2Funnamed_1_qostnv.gif)

Looking at it this way, as arrays of numbers, makes it a bit difficult for us to find any particular pattern that would differentiate clapping your hands from snapping your fingers for example.

To help us, we'd usually visualise this data. Two standard ways to do this include turning it into a waveform or frequency bar graph like below:

[![Data visualisation of sound input as a waveform and frequency bar graph](https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fres.cloudinary.com%2Fdevdevcharlie%2Fimage%2Fupload%2Fc_scale%2Cw_573%2Fv1576912920%2Faudio_ewnw0m_irmre1.gif)](https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fres.cloudinary.com%2Fdevdevcharlie%2Fimage%2Fupload%2Fc_scale%2Cw_573%2Fv1576912920%2Faudio_ewnw0m_irmre1.gif)

A waveform represents the sound wave’s displacement over time.

Sound being the vibration of air molecules, this graph shows the oscillation of a sound wave. But, visualised this way, we still can't really conclude anything.

A frequency bar graph shows you the sound data as a measure of how many times a waveform repeats in a given amount of time.

In this way of visualising, we could maybe start to gain some insights, recognise some kind of "beat", but we're still not quite there.

A better way to represent this data to find patterns is what is called a **spectrogram**.

**A spectrogram is a visual representation of the spectrum of frequencies of a signal as it varies with time. You can think of it as a heat map of sound.**

Using Canvas to visualise my microphone input as a spectrogram, I could identify pretty easily the difference between speaking and clapping my hands.

[![Spectrogram showing the difference between speaking and clapping my hands](https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fres.cloudinary.com%2Fdevdevcharlie%2Fimage%2Fupload%2Fv1575155392%2Fsound3_pn1nbt.gif)](https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fres.cloudinary.com%2Fdevdevcharlie%2Fimage%2Fupload%2Fv1575155392%2Fsound3_pn1nbt.gif)

So far, none of this is using machine learning. I'm only using the Web Audio API to get data from the microphone and Canvas to turn it into a spectrogram.

Now that we can observe that certain activities produce data that "looks" different, we can move on to using machine learning and Tensorflow.js to build a classification model.

## Classifying sound data using machine learning

As mentioned above, to make it easier, I used the Teachable machine experiment to record my sound samples, run the training and generate the model.

My main goal so far was to validate that my idea was feasible, so I preferred using something that was already built, however, you could definitely create your own sound classification system from scratch (I'd like that to be one of my potential next steps).

For now, my training process looked like this:

[![Training teachable machine with sound samples](https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fthepracticaldev.s3.amazonaws.com%2Fi%2F4k4ocym67h73lrqw4rc1.png)](https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fthepracticaldev.s3.amazonaws.com%2Fi%2F4k4ocym67h73lrqw4rc1.png)

First, you need to record some background noise for 20 seconds. This is a necessary step so the algorithm would recognise some kind of neutral state when you're not doing any activity.

Then, you can add different "classes" which are your different activities. You need to record a minimum of 8 samples / activity but the more the better.

In the example above, I only record samples for snapping my fingers, then run the training and check the predictions live at the end.

Depending on what you would like to build, you might add a lot more classes, but it's important to check if the prediction is accurate so you can record more samples and re-train if needed.

If you're happy with the output, you can download the model and use it in your project.

## Using the machine learning model

Once the training process is done, you can use the model generated to run some live predictions with new sound samples it has never "seen" before.

To do this, you need to start by importing the framework and another model:

```
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.3.1/dist/tf.min.js">
</script>

<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/speech-commands@0.4.0/dist/speech-commands.min.js">
</script>

```

Enter fullscreen modeExit fullscreen mode

In the code sample above, we import Tensorflow.js and the [speech-commands model](https://github.com/tensorflow/tfjs-models/tree/master/speech-commands).

We need to do this because the way we are predicting new sound samples is done using **transfer learning**.

Transfer learning means that we're using a pre-trained model that is optimised with a certain kind of input, adding our own samples to it and re-training everything together.

The _speech-commands_ model is the only pre-trained model available with Tensorflow.js that has been trained with sound samples.

It is optimised to classify spoken words, but I wanted to see if it could still be accurate being given sound samples of activities.

Once you've imported the tools, you need to load your custom model:

```
let model;

async function setupModel(URL, predictionCB) {
   //store the prediction and audio callback functions
   predictionCallback = predictionCB;

   const modelURL = 'model.json';
   const metadataURL = 'metadata.json';
   model = window.speechCommands.create('BROWSER_FFT', undefined, modelURL, metadataURL);
   await model.ensureModelLoaded();

   const modelParameters = {
       invokeCallbackOnNoiseAndUnknown: true, // run even when only background noise is detected
       includeSpectrogram: true, // give us access to numerical audio data
       overlapFactor: 0.5 // how often per second to sample audio, 0.5 means twice per second
   };

   model.listen(
       //This callback function is invoked each time the model has a prediction.
       prediction => {
           predictionCallback(prediction.scores);
       },
       modelParameters
   );
}

```

Enter fullscreen modeExit fullscreen mode

When you download your model from Teachable machine, you get a `model.json` and `metadata.json` files. You need both for it to work. The metadata.json file contains information about the name of your classes, etc...

Then, you need to invoke the `speechCommands` model and pass it your model variables.

Once the model is loaded, you can define a few extra parameters, call the `listen` method that will trigger a callback every time it has predicted something from the live audio data coming from the microphone.

Once your function is set up, you can call it this way:

```
let labels = ["Clapping","Speaking","_background_noise_"];

setupModel(URL, data => {
     // data will look like this [0.87689, 0.21456, 0.56789]
      switch(Math.max(...data)){
               case data[0]:
                   currentPrediction = labels[0];
                   break;
               case data[1]:
                   currentPrediction = labels[1];
                   break;
               default:
                   currentPrediction = "";
                   break;
           }
       }
        return currentPrediction;
});

```

Enter fullscreen modeExit fullscreen mode

I defined an array with classes that I trained and, when a prediction happens, the result will come back as an array of floats between 0 and 1, representing the probability of each class to be the one predicted; if the maximum number is the first in the array of probabilities, our activity recognised will be the 1st in our labels array defined above.

## Demo

In the end, my prototype looks something like this:

[![Activity classification between speaking, coughing and brushing my teeth](https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fres.cloudinary.com%2Fdevdevcharlie%2Fimage%2Fupload%2Fv1575157529%2Factivities2_wzux1a.gif)](https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fres.cloudinary.com%2Fdevdevcharlie%2Fimage%2Fupload%2Fv1575157529%2Factivities2_wzux1a.gif)

If you want to play around with it yourself, here's the [link to the demo](https://acoustic-ml.netlify.com/).

It will be more accurate if you try it in a quiet environment because I recorded my samples at home. If you try it in the train or in a cafe, the background noise will be too different from the one provided for the training so the accuracy will drop.

_At the moment, because of time restrictions, it's mostly optimised for Chrome on desktop. There's a few things to fix up for it to work as expected on other browsers and mobile.😞_

## Applications

For now, this prototype is only exploratory but I believe there is potential to this kind of technology.

Instead of buying multiple expensive smart devices such as fridges, coffee machines and microwaves, that are only aware of themselves (a smart fridge doesn't know if the coffee machine is on, etc...), we could replace them with a single device that would have more contextual understanding, not only of other devices, but of which room it is in and of its users' activities.

This could help with the following applications.

### Contextually-aware video services

#### Cooking

If you're following a recipe on Youtube while you're cooking, the video could pause automatically when you are supposed to chop some vegetables, or microwave something for a minute, or use a whisk, etc... by listening to your activities. This way, you wouldn't have to go back and forth between your laptop, tablet or phone while cooking.

#### Watching your favourite TV series

If you're watching Netflix and your phone or doorbell rings, the video could be paused without you having to find the remote because a doorbell or phone ringing usually means you're gonna be away from the TV for a few minutes.

### Interactive story-telling

If you're working at a creative agency or media company where engagement with your online content is important, this type of technology could mean a more interactive and entertaining way to consume content. A story could be a lot more immersive by asking you to participate in it in different ways, by clapping your hands, imitating some sounds, etc...

### Health tracking

As you could see in my quick demo, you can classify the sound of coughing and brushing your teeth, so you could also train your system to recognise sneezing, snoring, etc... and build some kind of health tracking system.

There are definitely a lot more applications but this was just to give you an idea of where it could be used.

## Limitations

One of the main limits would be privacy concerns. You wouldn't want your Google Home or Amazon Alexa to, not only listen to your conversations, but also know everything you're doing by listening to all your activities... right???! 😢

There would be a way to build your own private offline system but that is definitely not something most people would have access to.

The other main limitation at the moment is the inability to understand multiple activities at once. If you are brushing your teeth while someone is talking to you, this system would only predict a single activity at a time.

However, this is where another exciting project comes into play, which is called " [General-purpose synthetic sensors](http://www.gierad.com/assets/supersensor/supersensor.pdf)" 😍, that I'll talk about briefly next.

## Next steps

I still have a lot more to learn about this and I'm hoping to have the time to explore that space in 2020, but one of my next step would be to build [general-purpose synthetic sensors](http://www.gierad.com/assets/supersensor/supersensor.pdf).

Instead of only using sound data to recognise activities, researchers at the CMU also worked on a project to create a small device integrating several sensors (microphone, temperature sensor, accelerometer, motion sensor, light sensor, etc...), to combine multiple spectrograms, in the aim to monitor larger contexts and build a more robust activity recognition system.

[![General-purpose synthetic sensor with spectrograms](https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fthepracticaldev.s3.amazonaws.com%2Fi%2Fv2425615u9zyyy06nchh.png)](https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fthepracticaldev.s3.amazonaws.com%2Fi%2Fv2425615u9zyyy06nchh.png)

* * *

That's it for now!

I understand that people might be skeptical about this experiment, especially in JavaScript, but knowing that no hardware upgrade is needed for this to work, means that the main thing we're waiting for might be finding the right application.

Personally, I'm always super excited to know that, as developers, we can take part in such research by building prototypes using JavaScript and explore what might be the future of interactions.

Hope it helps! 🙂

[![profile](https://media2.dev.to/dynamic/image/width=64,height=64,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Forganization%2Fprofile_image%2F123%2F38b10714-65da-4f1d-88ae-e9b28c1d7a5e.png)\\
Heroku](https://dev.to/heroku) Promoted

Dropdown menu

- [What's a billboard?](https://dev.to/billboards)
- [Manage preferences](https://dev.to/settings/customization#sponsors)

* * *

- [Report billboard](https://dev.to/report-abuse?billboard=217497)

[![Heroku](https://media2.dev.to/dynamic/image/width=775%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fi.imgur.com%2Fq0bF1JF.png)](https://signup.heroku.com/?utm_source=devto&utm_medium=paid&utm_campaign=heroku_2025&bb=217497)

## [Deploy with ease. Manage efficiently. Scale faster.](https://signup.heroku.com/?utm_source=devto&utm_medium=paid&utm_campaign=heroku_2025&bb=217497)

Leave the infrastructure headaches to us, while you focus on pushing boundaries, realizing your vision, and making a lasting impression on your users.

[Get Started](https://signup.heroku.com/?utm_source=devto&utm_medium=paid&utm_campaign=heroku_2025&bb=217497)

Read More


![pic](https://media2.dev.to/dynamic/image/width=256,height=,fit=scale-down,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F8j7kvp660rqzt99zui8e.png)

[Create template](https://dev.to/settings/response-templates)

Templates let you quickly answer FAQs or store snippets for re-use.

SubmitPreview [Dismiss](https://dev.to/404.html)

CollapseExpand

[![supunkavinda profile image](https://media2.dev.to/dynamic/image/width=50,height=50,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F180627%2F4a4bcefb-1074-4b56-98ce-6738ff80c3d2.jpeg)](https://dev.to/supunkavinda)

[Supun Kavinda](https://dev.to/supunkavinda)

Supun Kavinda



[![](https://media2.dev.to/dynamic/image/width=90,height=90,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F180627%2F4a4bcefb-1074-4b56-98ce-6738ff80c3d2.jpeg)\\
Supun Kavinda](https://dev.to/supunkavinda)

Follow

Programming


- Email


[supunkavinda1125@gmail.com](mailto:supunkavinda1125@gmail.com)

- Location



France


- Joined


Jun 14, 2019


• [Dec 22 '19](https://dev.to/devdevcharlie/acoustic-activity-recognition-in-javascript-2go4#comment-j8j5)

Dropdown menu

- [Copy link](https://dev.to/devdevcharlie/acoustic-activity-recognition-in-javascript-2go4#comment-j8j5)
- Hide

- [Report abuse](https://dev.to/report-abuse?url=https://dev.to/supunkavinda/comment/j8j5)

Okay, one question.

low-frequency whistling = brushing teeth.

How does this happen? Does the software recognize frequency, or it also recognizes the tone?

CollapseExpand

[![devdevcharlie profile image](https://media2.dev.to/dynamic/image/width=50,height=50,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F22924%2Fd8b205e7-f66a-4d59-b960-ba4c838db516.png)](https://dev.to/devdevcharlie)

[Charlie Gerard](https://dev.to/devdevcharlie)

Charlie Gerard



[![](https://media2.dev.to/dynamic/image/width=90,height=90,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F22924%2Fd8b205e7-f66a-4d59-b960-ba4c838db516.png)\\
Charlie Gerard](https://dev.to/devdevcharlie)

Follow

I am a senior developer advocate, passionate about creative coding and building interactive prototypes mixing science, art & technology. I also spend time mentoring, contributing to OSS and speaking.


- Location



Seattle


- Work



Senior Developer Advocate at Stripe


- Joined


Jun 23, 2017


• [Dec 24 '19](https://dev.to/devdevcharlie/acoustic-activity-recognition-in-javascript-2go4#comment-jan7)

Dropdown menu

- [Copy link](https://dev.to/devdevcharlie/acoustic-activity-recognition-in-javascript-2go4#comment-jan7)
- Hide

- [Report abuse](https://dev.to/report-abuse?url=https://dev.to/devdevcharlie/comment/jan7)

I didn't train it with whistling so it's predicting whatever is closest to what it was trained with, which must be brushing teeth in this case.

It's looking at all the data coming from the WebAudio API

CollapseExpand

[![supunkavinda profile image](https://media2.dev.to/dynamic/image/width=50,height=50,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F180627%2F4a4bcefb-1074-4b56-98ce-6738ff80c3d2.jpeg)](https://dev.to/supunkavinda)

[Supun Kavinda](https://dev.to/supunkavinda)

Supun Kavinda



[![](https://media2.dev.to/dynamic/image/width=90,height=90,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F180627%2F4a4bcefb-1074-4b56-98ce-6738ff80c3d2.jpeg)\\
Supun Kavinda](https://dev.to/supunkavinda)

Follow

Programming


- Email


[supunkavinda1125@gmail.com](mailto:supunkavinda1125@gmail.com)

- Location



France


- Joined


Jun 14, 2019


• [Dec 24 '19](https://dev.to/devdevcharlie/acoustic-activity-recognition-in-javascript-2go4#comment-jana)

Dropdown menu

- [Copy link](https://dev.to/devdevcharlie/acoustic-activity-recognition-in-javascript-2go4#comment-jana)
- Hide

- [Report abuse](https://dev.to/report-abuse?url=https://dev.to/supunkavinda/comment/jana)

Yep. I asked if it is trained for frequency or the tone? or both?

[![devdevcharlie profile image](https://media2.dev.to/dynamic/image/width=50,height=50,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F22924%2Fd8b205e7-f66a-4d59-b960-ba4c838db516.png)](https://dev.to/devdevcharlie)

[Charlie Gerard](https://dev.to/devdevcharlie)

Charlie Gerard



[![](https://media2.dev.to/dynamic/image/width=90,height=90,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F22924%2Fd8b205e7-f66a-4d59-b960-ba4c838db516.png)\\
Charlie Gerard](https://dev.to/devdevcharlie)

Follow

I am a senior developer advocate, passionate about creative coding and building interactive prototypes mixing science, art & technology. I also spend time mentoring, contributing to OSS and speaking.


- Location



Seattle


- Work



Senior Developer Advocate at Stripe


- Joined


Jun 23, 2017


• [Dec 24 '19](https://dev.to/devdevcharlie/acoustic-activity-recognition-in-javascript-2go4#comment-jann)

Dropdown menu

- [Copy link](https://dev.to/devdevcharlie/acoustic-activity-recognition-in-javascript-2go4#comment-jann)
- Hide

- [Report abuse](https://dev.to/report-abuse?url=https://dev.to/devdevcharlie/comment/jann)

If by tone you mean notes, notes are named frequencies.

The data you get back from the WebAudio API is the frequencies picked up by the microphone copied into a Uint8Array using the getFrequencyByteData method on the AnalyserNode.

[![supunkavinda profile image](https://media2.dev.to/dynamic/image/width=50,height=50,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F180627%2F4a4bcefb-1074-4b56-98ce-6738ff80c3d2.jpeg)](https://dev.to/supunkavinda)

[Supun Kavinda](https://dev.to/supunkavinda)

Supun Kavinda



[![](https://media2.dev.to/dynamic/image/width=90,height=90,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F180627%2F4a4bcefb-1074-4b56-98ce-6738ff80c3d2.jpeg)\\
Supun Kavinda](https://dev.to/supunkavinda)

Follow

Programming


- Email


[supunkavinda1125@gmail.com](mailto:supunkavinda1125@gmail.com)

- Location



France


- Joined


Jun 14, 2019


• [Dec 24 '19](https://dev.to/devdevcharlie/acoustic-activity-recognition-in-javascript-2go4#comment-jap2)

Dropdown menu

- [Copy link](https://dev.to/devdevcharlie/acoustic-activity-recognition-in-javascript-2go4#comment-jap2)
- Hide

- [Report abuse](https://dev.to/report-abuse?url=https://dev.to/supunkavinda/comment/jap2)

By tone, I meant tone: which makes a C note from violin and piano distinct.

However, I got what you meant. Nice and creative work!

[![josepjaume profile image](https://media2.dev.to/dynamic/image/width=50,height=50,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F235569%2F7b05a5b4-729e-4a79-9256-4cfa2d594a26.jpg)](https://dev.to/josepjaume)

[Josep Jaume Rey](https://dev.to/josepjaume)

Josep Jaume Rey



[![](https://media2.dev.to/dynamic/image/width=90,height=90,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F235569%2F7b05a5b4-729e-4a79-9256-4cfa2d594a26.jpg)\\
Josep Jaume Rey](https://dev.to/josepjaume)

Follow

- Location



Barcelona


- Work



Web Developer at Codegram


- Joined


Sep 24, 2019


• [Jun 8 '20](https://dev.to/devdevcharlie/acoustic-activity-recognition-in-javascript-2go4#comment-104dh)

Dropdown menu

- [Copy link](https://dev.to/devdevcharlie/acoustic-activity-recognition-in-javascript-2go4#comment-104dh)
- Hide

- [Report abuse](https://dev.to/report-abuse?url=https://dev.to/josepjaume/comment/104dh)

Notes are just particular frequencies that are given a name. In any case, the _tone_ you're using is a bit off.

[![techpeace profile image](https://media2.dev.to/dynamic/image/width=50,height=50,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F183313%2F52656b54-61c3-4a54-9619-b974790d4120.jpeg)](https://dev.to/techpeace)

[Matt Buck](https://dev.to/techpeace)

Matt Buck



[![](https://media2.dev.to/dynamic/image/width=90,height=90,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F183313%2F52656b54-61c3-4a54-9619-b974790d4120.jpeg)\\
Matt Buck](https://dev.to/techpeace)

Follow

I make machines talk.


- Location



Austin


- Work



Co-founder & CTO, full-stack voice developer at Voxable


- Joined


Jun 19, 2019


• [Sep 30 '20](https://dev.to/devdevcharlie/acoustic-activity-recognition-in-javascript-2go4#comment-15g78)

Dropdown menu

- [Copy link](https://dev.to/devdevcharlie/acoustic-activity-recognition-in-javascript-2go4#comment-15g78)
- Hide

- [Report abuse](https://dev.to/report-abuse?url=https://dev.to/techpeace/comment/15g78)

Traditionally, that's referred to as [timbre](https://en.wikipedia.org/wiki/Timbre). It's the volume of specific frequencies in a note's harmonic series that account for differences in timbre in pitched instruments. [Here's a great video explaining how that works.](https://www.youtube.com/watch?v=Wx_kugSemfY)

CollapseExpand

[![veselinastaneva profile image](https://media2.dev.to/dynamic/image/width=50,height=50,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F215430%2F6c3d4f28-d4f6-4f0a-aa54-d035b626382b.jpg)](https://dev.to/veselinastaneva)

[Vesi Staneva](https://dev.to/veselinastaneva)

Vesi Staneva



[![](https://media2.dev.to/dynamic/image/width=90,height=90,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F215430%2F6c3d4f28-d4f6-4f0a-aa54-d035b626382b.jpg)\\
Vesi Staneva](https://dev.to/veselinastaneva)

Follow

Firm believer that behind every ⛔ sign, there is a door. Expert in getting 💩 done. Love getting in touch with like-minded people and talking "change"!


- Location



Sofia, Bulgaria


- Work



Business Development & Marketing Manager at SashiDo.io


- Joined


Aug 20, 2019


• [Jul 3 '20](https://dev.to/devdevcharlie/acoustic-activity-recognition-in-javascript-2go4#comment-11bjc)

Dropdown menu

- [Copy link](https://dev.to/devdevcharlie/acoustic-activity-recognition-in-javascript-2go4#comment-11bjc)
- Hide

- [Report abuse](https://dev.to/report-abuse?url=https://dev.to/veselinastaneva/comment/11bjc)

Awesome post! Kudos for making such an interesting experiment with JavaScript 👏 👏 👏

Actually, my team just completed an open-sourced [Content Moderation Service](https://bit.ly/31H4Vxa) built with Node.js, TensorFlowJS, and ReactJS that we have been working over the past weeks. We have now released the first part of a series of three tutorials - [How to create an NSFW Image Classification REST API](https://bit.ly/3gqMt0b) and we would love to hear your feedback(no ML experience needed to get it working). Any comments & suggestions are more than welcome. Thanks in advance!

( [Fork it on GitHub or click🌟star](https://bit.ly/3glDddC) to support us and stay connected🙌)

CollapseExpand

[![brunooliveira profile image](https://media2.dev.to/dynamic/image/width=50,height=50,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F209943%2F99a54cfd-2319-4620-adfb-891887292175.jpeg)](https://dev.to/brunooliveira)

[Bruno Oliveira](https://dev.to/brunooliveira)

Bruno Oliveira



[![](https://media2.dev.to/dynamic/image/width=90,height=90,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F209943%2F99a54cfd-2319-4620-adfb-891887292175.jpeg)\\
Bruno Oliveira](https://dev.to/brunooliveira)

Follow

- Location



Amsterdam


- Work



Software Developer at SIG


- Joined


Aug 10, 2019


• [Dec 22 '19](https://dev.to/devdevcharlie/acoustic-activity-recognition-in-javascript-2go4#comment-j8f0)

Dropdown menu

- [Copy link](https://dev.to/devdevcharlie/acoustic-activity-recognition-in-javascript-2go4#comment-j8f0)
- Hide

- [Report abuse](https://dev.to/report-abuse?url=https://dev.to/brunooliveira/comment/j8f0)

Amazing!! Your side projects are always awesome

CollapseExpand

[![mvasigh profile image](https://media2.dev.to/dynamic/image/width=50,height=50,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F121627%2F59829a58-6568-4329-a922-7e120607b58b.png)](https://dev.to/mvasigh)

[Mehdi Vasigh](https://dev.to/mvasigh)

Mehdi Vasigh



[![](https://media2.dev.to/dynamic/image/width=90,height=90,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F121627%2F59829a58-6568-4329-a922-7e120607b58b.png)\\
Mehdi Vasigh](https://dev.to/mvasigh)

Follow

Software Engineer @Arundo, Instructor @GA


- Location



Houston, TX


- Work



Senior Software Engineer at Arundo Analytics


- Joined


Dec 13, 2018


• [Dec 23 '19](https://dev.to/devdevcharlie/acoustic-activity-recognition-in-javascript-2go4#comment-j9ca)

Dropdown menu

- [Copy link](https://dev.to/devdevcharlie/acoustic-activity-recognition-in-javascript-2go4#comment-j9ca)
- Hide

- [Report abuse](https://dev.to/report-abuse?url=https://dev.to/mvasigh/comment/j9ca)

This is super cool! An awesome intersection of a bunch of different things I've been wanting to play with. Thanks for putting this together.

CollapseExpand

[![codermikky profile image](https://media2.dev.to/dynamic/image/width=50,height=50,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F143707%2F6c8dce1e-de90-44ca-9d76-14911bbd0bf1.png)](https://dev.to/codermikky)

[Sai Deep Konduri](https://dev.to/codermikky)

Sai Deep Konduri



[![](https://media2.dev.to/dynamic/image/width=90,height=90,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F143707%2F6c8dce1e-de90-44ca-9d76-14911bbd0bf1.png)\\
Sai Deep Konduri](https://dev.to/codermikky)

Follow

UI developer aspiring ML engineer


- Location



Hyderabad


- Work



Consultant at Deloitte


- Joined


Mar 9, 2019


• [Dec 23 '19](https://dev.to/devdevcharlie/acoustic-activity-recognition-in-javascript-2go4#comment-j8pg)

Dropdown menu

- [Copy link](https://dev.to/devdevcharlie/acoustic-activity-recognition-in-javascript-2go4#comment-j8pg)
- Hide

- [Report abuse](https://dev.to/report-abuse?url=https://dev.to/codermikky/comment/j8pg)

Impressive

CollapseExpand

[![user1m profile image](https://media2.dev.to/dynamic/image/width=50,height=50,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F216211%2Fe2a5f8e1-bace-4849-b943-4c00d1d4d15b.png)](https://dev.to/user1m)

[Claudius Mbemba](https://dev.to/user1m)

Claudius Mbemba



[![](https://media2.dev.to/dynamic/image/width=90,height=90,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F216211%2Fe2a5f8e1-bace-4849-b943-4c00d1d4d15b.png)\\
Claudius Mbemba](https://dev.to/user1m)

Follow

- Location



Seattle WA


- Work



Chief Coder at Neu, Inc


- Joined


Aug 21, 2019


• [Dec 29 '19](https://dev.to/devdevcharlie/acoustic-activity-recognition-in-javascript-2go4#comment-jf77)

Dropdown menu

- [Copy link](https://dev.to/devdevcharlie/acoustic-activity-recognition-in-javascript-2go4#comment-jf77)
- Hide

- [Report abuse](https://dev.to/report-abuse?url=https://dev.to/user1m/comment/jf77)

Really cool [@devdevcharlie](https://dev.to/devdevcharlie)
!

CollapseExpand

[![husslerrr profile image](https://media2.dev.to/dynamic/image/width=50,height=50,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F303962%2F1839b70c-5c80-4d8b-8762-bf44d0bf5043.jpg)](https://dev.to/husslerrr)

[Brown-West](https://dev.to/husslerrr)

Brown-West



[![](https://media2.dev.to/dynamic/image/width=90,height=90,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F303962%2F1839b70c-5c80-4d8b-8762-bf44d0bf5043.jpg)\\
Brown-West](https://dev.to/husslerrr)

Follow

- Joined


Dec 29, 2019


• [Dec 29 '19](https://dev.to/devdevcharlie/acoustic-activity-recognition-in-javascript-2go4#comment-jf9g)

Dropdown menu

- [Copy link](https://dev.to/devdevcharlie/acoustic-activity-recognition-in-javascript-2go4#comment-jf9g)
- Hide

- [Report abuse](https://dev.to/report-abuse?url=https://dev.to/husslerrr/comment/jf9g)

Nice work, trying to remake this for practice purposes, how do I go about it ?

CollapseExpand

[![anthonyofboston profile image](https://media2.dev.to/dynamic/image/width=50,height=50,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F2519744%2Fb72a6df0-9b00-4616-8f8a-f04f3c094a96.png)](https://dev.to/anthonyofboston)

[Anthony](https://dev.to/anthonyofboston)

Anthony



[![](https://media2.dev.to/dynamic/image/width=90,height=90,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F2519744%2Fb72a6df0-9b00-4616-8f8a-f04f3c094a96.png)\\
Anthony](https://dev.to/anthonyofboston)

Follow

App making and astrology. Interested in object and audio detection.


- Joined


Dec 3, 2024


• [Dec 12 '24](https://dev.to/devdevcharlie/acoustic-activity-recognition-in-javascript-2go4#comment-2k8p9)

Dropdown menu

- [Copy link](https://dev.to/devdevcharlie/acoustic-activity-recognition-in-javascript-2go4#comment-2k8p9)
- Hide

- [Report abuse](https://dev.to/report-abuse?url=https://dev.to/anthonyofboston/comment/2k8p9)

Thank you. I used this tutorial and was able to create my own acoustics activity recognition app for detecting drones [anthonyofboston.github.io/](https://anthonyofboston.github.io/)

[View full discussion (14 comments)](https://dev.to/devdevcharlie/acoustic-activity-recognition-in-javascript-2go4/comments)

Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's [permalink](https://dev.to/devdevcharlie/acoustic-activity-recognition-in-javascript-2go4#).


Hide child comments as well

Confirm


For further actions, you may consider blocking this person and/or [reporting abuse](https://dev.to/report-abuse)

[![profile](https://media2.dev.to/dynamic/image/width=64,height=64,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Forganization%2Fprofile_image%2F3774%2F02d4162c-978f-4471-9d39-b2928cfb9e24.png)\\
Sentry](https://dev.to/sentry) Promoted

Dropdown menu

- [What's a billboard?](https://dev.to/billboards)
- [Manage preferences](https://dev.to/settings/customization#sponsors)

* * *

- [Report billboard](https://dev.to/report-abuse?billboard=69745)

[![nextjs tutorial video](https://media2.dev.to/dynamic/image/width=775%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fm0hi8n3i4dbgdoc0itzk.jpg)](https://bit.ly/nextjs-distributed-tracing?bb=69745)

## [Youtube Tutorial Series 📺](https://bit.ly/nextjs-distributed-tracing?bb=69745)

So you built a Next.js app, but you need a clear view of the entire operation flow to be able to identify performance bottlenecks before you launch. But how do you get started? Get the essentials on tracing for Next.js from [@nikolovlazar](https://dev.to/nikolovlazar?bb=69745) in this video series 👀

[Watch the Youtube series](https://bit.ly/nextjs-distributed-tracing?bb=69745)

## Read next

[![mikeyoung44 profile image](https://media2.dev.to/dynamic/image/width=100,height=100,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F1054351%2F445fb057-59a3-41ac-81ec-9d0c93a5c618.jpg)\\
\\
**DeepSeek R1: Math Model Trades Speed for Accuracy in Complex Problem-Solving**\\
\\
Mike Young - Feb 3](https://dev.to/mikeyoung44/deepseek-r1-math-model-trades-speed-for-accuracy-in-complex-problem-solving-47o1) [![mikeyoung44 profile image](https://media2.dev.to/dynamic/image/width=100,height=100,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F1054351%2F445fb057-59a3-41ac-81ec-9d0c93a5c618.jpg)\\
\\
**New Context Pruning Method Cuts AI Memory Use by 50% While Maintaining Accuracy**\\
\\
Mike Young - Feb 3](https://dev.to/mikeyoung44/new-context-pruning-method-cuts-ai-memory-use-by-50-while-maintaining-accuracy-122g) [![mikeyoung44 profile image](https://media2.dev.to/dynamic/image/width=100,height=100,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F1054351%2F445fb057-59a3-41ac-81ec-9d0c93a5c618.jpg)\\
\\
**AI Models Still Fail Basic Physics Tests, New Benchmark Shows 18.4% Improvement Possible**\\
\\
Mike Young - Feb 3](https://dev.to/mikeyoung44/ai-models-still-fail-basic-physics-tests-new-benchmark-shows-184-improvement-possible-3125) [![dpc profile image](https://media2.dev.to/dynamic/image/width=100,height=100,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F2488802%2F41f37128-98c4-4c18-a30a-daa00225f14c.png)\\
\\
**Daily JavaScript Challenge \#JS-102: Calculate Unique Elements in a Square Matrix**\\
\\
DPC - Feb 14](https://dev.to/dpc/daily-javascript-challenge-js-102-calculate-unique-elements-in-a-square-matrix-2a9d)

👋 Kindness is contagious

Dropdown menu

- [What's a billboard?](https://dev.to/billboards)
- [Manage preferences](https://dev.to/settings/customization#sponsors)

* * *

- [Report billboard](https://dev.to/report-abuse?billboard=217483)

Close

Please show some love ❤️ or share a kind word in the comments if you found this useful!

### [Got it!](https://dev.to/enter?state=new-user&bb=217483)

![DEV Community](https://media2.dev.to/dynamic/image/width=190,height=,fit=scale-down,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F8j7kvp660rqzt99zui8e.png)

We're a place where coders share, stay up-to-date and grow their careers.


[Log in](https://dev.to/enter) [Create account](https://dev.to/enter?state=new-user)

![](https://assets.dev.to/assets/sparkle-heart-5f9bee3767e18deb1bb725290cb151c25234768a0e9a2bd39370c382d02920cf.svg)![](https://assets.dev.to/assets/multi-unicorn-b44d6f8c23cdd00964192bedc38af3e82463978aa611b4365bd33a0f1f4f3e97.svg)![](https://assets.dev.to/assets/exploding-head-daceb38d627e6ae9b730f36a1e390fca556a4289d5a41abb2c35068ad3e2c4b5.svg)![](https://assets.dev.to/assets/raised-hands-74b2099fd66a39f2d7eed9305ee0f4553df0eb7b4f11b01b6b1b499973048fe5.svg)![](https://assets.dev.to/assets/fire-f60e7a582391810302117f987b22a8ef04a2fe0df7e3258a5f49332df1cec71e.svg)

[iframe](https://forem.com/auth_pass/iframe)